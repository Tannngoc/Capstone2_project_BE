import os
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from keras.src.models import Sequential
from keras.src.layers import LSTM, Dense, Dropout

def load_all_stock_closes(folder_path):
    close_data = []
    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]
    
    for file in csv_files:
        path = os.path.join(folder_path, file)
        df = pd.read_csv(path)
        
        if 'Date' not in df.columns and 'Price' in df.columns:
            df.rename(columns={'Price': 'Date'}, inplace=True)

        if 'Date' in df.columns and 'Close' in df.columns:
            df['Date'] = pd.to_datetime(df['Date'], format="%Y-%m-%d")
            df.set_index('Date', inplace=True)
            close_data.append(df[['Close']].rename(columns={'Close': file.split('_')[0]}))
        else:
            print(f"‚ö†Ô∏è File {file} thi·∫øu c·ªôt 'Date' ho·∫∑c 'Close', b·ªè qua.")

    if not close_data:
        raise ValueError("‚ùó Kh√¥ng c√≥ file h·ª£p l·ªá ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh.")
    
    merged = pd.concat(close_data, axis=1)
    merged.dropna(inplace=True)
    return merged

def train_joint_model(data):
    scaler = MinMaxScaler(feature_range=(0, 0.9))
    scaled_data = scaler.fit_transform(data)

    training_data_len = int(len(scaled_data) * 0.95)
    train_data = scaled_data[:training_data_len]

    x_train, y_train = [], []

    for i in range(60, len(train_data)):
        x_train.append(train_data[i-60:i])
        y_train.append(train_data[i])

    x_train, y_train = np.array(x_train), np.array(y_train)

    model = Sequential()
    model.add(LSTM(128, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))
    model.add(Dropout(0.2))
    model.add(LSTM(64, return_sequences=False))
    model.add(Dropout(0.2))
    model.add(Dense(y_train.shape[1]))

    model.compile(optimizer='adam', loss='mean_squared_error')
    model.fit(x_train, y_train, batch_size=32, epochs=20, verbose=1)

    # D·ª± ƒëo√°n ƒë·ªÉ ƒë√°nh gi√° RMSE
    test_data = scaled_data[training_data_len - 60:]
    x_test = [test_data[i-60:i] for i in range(60, len(test_data))]
    x_test = np.array(x_test)

    predictions = model.predict(x_test)
    predictions = scaler.inverse_transform(predictions)
    actual = data[training_data_len:].values

    rmse = np.sqrt(np.mean((predictions - actual) ** 2))
    print(f"‚úÖ RMSE to√†n t·∫≠p: {rmse:.2f}")

    # === L∆∞u m√¥ h√¨nh v·ªÅ th∆∞ m·ª•c AI ===
    model_path = "app/AI/joint_stock_model.keras"
    model_dir = os.path.dirname(model_path)

    if not os.path.exists(model_dir):
        os.makedirs(model_dir)

    if os.path.exists(model_path):
        os.remove(model_path)
        print("üßπ ƒê√£ x√≥a model c≈©.")

    model.save(model_path)
    print("‚úÖ M√¥ h√¨nh m·ªõi ƒë√£ ƒë∆∞·ª£c l∆∞u th√†nh c√¥ng!")

    return model, scaler, predictions, actual, data[training_data_len:]

# === Main logic ===
if __name__ == "__main__":
    print("üöÄ ƒêang load d·ªØ li·ªáu...")
    data = load_all_stock_closes("app/db")
    print("‚úÖ D·ªØ li·ªáu ƒë√£ s·∫µn s√†ng. ƒêang hu·∫•n luy·ªán m√¥ h√¨nh...")
    train_joint_model(data)
